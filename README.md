# Medical RAG Chatbot 
An endâ€‘toâ€‘end medical questionâ€‘answering chatbot built using Retrievalâ€‘Augmented Generation (RAG).

# ðŸ©º Medi-Chat RAG

**Medi-Chat RAG** is an **educational medical question-answering chatbot** built using **Retrieval-Augmented Generation (RAG)**.
It allows users to ask medical-related questions and receive answers grounded in uploaded PDF documents.

âš ï¸ **Important Disclaimer**
This project is for **educational and research purposes only**.
It **does NOT provide medical advice**.
Always consult a **qualified doctor or healthcare professional** for diagnosis and treatment decisions.

---

##  Features

*  Ingests **medical PDF documents**
*  Semantic search using **ChromaDB**
*  RAG pipeline with **LangChain**
*  Local LLM inference using **Mistral-7B (4-bit quantized)**
*  Backend API with **FastAPI**
*  Chat-based frontend using **Streamlit**
*  GPU acceleration (CUDA supported)

---

## Project Structure

```
medi-chat-rag/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ helper.py        # RAG pipeline logic
â”‚   â””â”€â”€ prompt.py        # Prompt template with disclaimer
â”‚
â”œâ”€â”€ research/
â”‚   â”œâ”€â”€ chroma_db/       # Persistent Chroma vector store
â”‚   â””â”€â”€ trials.ipynb     # Experiments & notebooks
â”‚
â”œâ”€â”€ data/                # Medical PDF documents
â”‚
â”œâ”€â”€ app.py               # Streamlit frontend
â”œâ”€â”€ server.py            # FastAPI backend
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ .env
â””â”€â”€ README.md
```

---

## RAG Architecture

1. **PDF Loading** â€“ Medical PDFs are loaded from the `data/` folder
2. **Chunking** â€“ Documents are split into overlapping chunks
3. **Embeddings** â€“ Generated using `sentence-transformers/all-MiniLM-L6-v2`
4. **Vector Store** â€“ Stored in **ChromaDB**
5. **Retriever** â€“ Fetches top-k relevant chunks
6. **LLM** â€“ Mistral-7B-Instruct (4-bit quantized)
7. **Prompt** â€“ Constrained, context-only medical QA
8. **Response** â€“ Short, safe, educational answers

---

##  Installation

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/jsuryanm/medi-chat-rag.git
cd medi-chat-rag
```

### 2ï¸âƒ£ Create Virtual Environment (Recommended)

```bash
conda create --name myenv python=3.11
conda activate myenv
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

>  **Note**
> CUDA-enabled PyTorch is configured for **CUDA 12.8**.
> Make sure your GPU and drivers support this.

---

## Preparing the Vector Database

1. Place your medical PDFs inside the `data/` folder
2. Run the backend once to build/load the Chroma vector store:

```bash
python server.py
```

The vector database will be persisted in:

```text
research/chroma_db/
```

---

## Running the Application

### Start FastAPI Backend

```bash
uvicorn server:app --reload
```

API runs at:

```
http://localhost:8000
```

### Start Streamlit Frontend

```bash
streamlit run app.py
```

App runs at:

```
http://localhost:8501
```

---

##  Example Usage

* Ask questions like:

  * *What are the symptoms of diabetes?*
  * *What is hypertension?*
* Answers are **strictly based on the uploaded PDFs**
* If information is unavailable, the bot responds:

  > *"I don't know"*

---

##  Safety & Disclaimer

* Uses **context-only answering**
* Limits answers to **3 concise sentences**
* Includes **explicit medical disclaimer**
* Encourages consultation with healthcare professionals

---

##  Tech Stack

* **Python**
* **FastAPI**
* **Streamlit**
* **LangChain**
* **ChromaDB**
* **HuggingFace Transformers**
* **Sentence Transformers**
* **PyTorch (CUDA)**

---

##  Future Improvements

* Authentication & user sessions
* Document upload via UI
* Source citation highlighting
* Multi-document collections
* Deployment with Docker
